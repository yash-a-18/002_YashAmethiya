{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Replace Manual version of Logistic Regression with TF based version."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import twitter_samples\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "nltk.download('twitter_samples')\r\n",
    "nltk.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re\r\n",
    "import string\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.tokenize import TweetTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\r\n",
    "def process_tweet(tweet):\r\n",
    "    \"\"\"Process tweet function.\r\n",
    "    Input:\r\n",
    "        tweet: a string containing a tweet\r\n",
    "    Output:\r\n",
    "        tweets_clean: a list of words containing the processed tweet\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    stemmer = PorterStemmer()\r\n",
    "    stopwords_english = stopwords.words('english')\r\n",
    "\r\n",
    "    # remove stock market tickers like $GE\r\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\r\n",
    "    # remove old style retweet text \"RT\"\r\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\r\n",
    "    # remove hyperlinks\r\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\r\n",
    "    # remove hashtags\r\n",
    "    # only removing the hash # sign from the word\r\n",
    "    tweet = re.sub(r'#', '', tweet)\r\n",
    "    # tokenize tweets\r\n",
    "\r\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\r\n",
    "                               reduce_len=True)\r\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\r\n",
    "\r\n",
    "    tweets_clean = []\r\n",
    "    for word in tweet_tokens:\r\n",
    "        # 1 remove stopwords\r\n",
    "        if word in stopwords_english:\r\n",
    "            continue\r\n",
    "        # 2 remove punctuation\r\n",
    "        if word in string.punctuation:\r\n",
    "            continue\r\n",
    "        # 3 stemming word\r\n",
    "        word = stemmer.stem(word)\r\n",
    "        # 4 Add it to tweets_clean\r\n",
    "        tweets_clean.append(word)\r\n",
    "\r\n",
    "    return tweets_clean"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\r\n",
    "# a positive label '1'         or\r\n",
    "# a negative label '0',\r\n",
    "\r\n",
    "#then builds the freqs dictionary, where each key is a (word,label) tuple,\r\n",
    "\r\n",
    "#and the value is the count of its frequency within the corpus of tweets.\r\n",
    "\r\n",
    "def build_freqs(tweets, ys):\r\n",
    "    \"\"\"Build frequencies.\r\n",
    "    Input:\r\n",
    "        tweets: a list of tweets\r\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\r\n",
    "            (either 0 or 1)\r\n",
    "    Output:\r\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\r\n",
    "        frequency\r\n",
    "    \"\"\"\r\n",
    "    # Convert np array to list since zip needs an iterable.\r\n",
    "    # The squeeze is necessary or the list ends up with one element.\r\n",
    "    # Also note that this is just a NOP if ys is already a list.\r\n",
    "    yslist = np.squeeze(ys).tolist()\r\n",
    "\r\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\r\n",
    "    # and over all processed words in each tweet.\r\n",
    "    freqs = {}\r\n",
    "\r\n",
    "    for y, tweet in zip(yslist, tweets):\r\n",
    "        for word in process_tweet(tweet):\r\n",
    "            pair = (word, y)\r\n",
    "\r\n",
    "            #############################################################\r\n",
    "            #Update the count of pair if present, set it to 1 otherwise\r\n",
    "            if pair in freqs:\r\n",
    "                freqs[pair] += 1\r\n",
    "            else:\r\n",
    "                freqs[pair] = 1\r\n",
    "\r\n",
    "    return freqs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# select the set of positive and negative tweets\r\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\r\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# split the data into two pieces, one for training and one for testing\r\n",
    "#############################################################\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "train_pos, test_pos = train_test_split(all_positive_tweets, test_size=0.2)\r\n",
    "train_neg, test_neg = train_test_split(all_negative_tweets, test_size=0.2)\r\n",
    "\r\n",
    "train_x = train_pos + train_neg\r\n",
    "test_x = test_pos + test_neg"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# combine positive and negative labels\r\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\r\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# create frequency dictionary\r\n",
    "#############################################################\r\n",
    "freqs = build_freqs(train_x, train_y)\r\n",
    "\r\n",
    "# check the output\r\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\r\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11328\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Example\r\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\r\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FF @dreamshakes @marketsmatter @MrsMarketUK @CheshireLadders @Streetzine @jewellery_bank @luvthenorth444 @BlueAppleHeroes thank you :-)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['ff', 'thank', ':-)']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def extract_features(tweet, freqs):\r\n",
    "    '''\r\n",
    "    Input:\r\n",
    "        tweet: a list of words for one tweet\r\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\r\n",
    "    Output:\r\n",
    "        x: a feature vector of dimension (1,3)\r\n",
    "    '''\r\n",
    "    # tokenizes, stems, and removes stopwords\r\n",
    "    #############################################################\r\n",
    "    output = []\r\n",
    "    for word_l in tweet:\r\n",
    "        word_l = process_tweet(word_l)\r\n",
    "\r\n",
    "        # 3 elements in the form of a 1 x 3 vector\r\n",
    "        x = np.zeros((1, 3))\r\n",
    "\r\n",
    "        #bias term is set to 1\r\n",
    "        x[0,0] = 1\r\n",
    "\r\n",
    "        # loop through each word in the list of words\r\n",
    "        for word in word_l:\r\n",
    "\r\n",
    "            # increment the word count for the positive label 1\r\n",
    "            x[0,1] += freqs.get((word, 1.0),0)\r\n",
    "\r\n",
    "            # increment the word count for the negative label 0\r\n",
    "            x[0,2] += freqs.get((word, 0.0),0)\r\n",
    "\r\n",
    "\r\n",
    "        assert(x.shape == (1, 3))\r\n",
    "        output.append(x)\r\n",
    "    return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "final_model = tf.keras.models.Sequential([\r\n",
    "    tf.keras.layers.Dense(2, activation=tf.nn.softmax)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "final_model.compile(optimizer='sgd',\r\n",
    "              loss='sparse_categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "final_model.fit(tf.convert_to_tensor(extract_features(train_x, freqs)), train_y, epochs=5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 33.4934 - accuracy: 0.9881\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 24.0159 - accuracy: 0.9927\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 0s 2ms/step - loss: 20.2874 - accuracy: 0.9931\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 17.1679 - accuracy: 0.9934\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 0s 1ms/step - loss: 13.1968 - accuracy: 0.9931\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2317fb3eaf0>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "final_model.evaluate(tf.convert_to_tensor(extract_features(test_x, freqs)), test_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 18.7141 - accuracy: 0.9945\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[18.714107513427734, 0.9944999814033508]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('MFE': conda)"
  },
  "interpreter": {
   "hash": "6cd9a464138fc6ccabc335c6117ac38c085d16cbe40298286e63ec121b3c4a6a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}